# 3.1 - 3.3 - ETC - ServeurIA_OpenWebUI

Vous devez configurer un serveur Open WebUi permettant d'utiliser modèles d'IA tel que Ollama via un interface web.

**Je sais, nous ne l'avons pas vu, le but est de vérifier si vous êtes capable d'exécuter une installation et configuration qui sort de ce que nous avons expérimenté.**

L'installation ce fait via un docker, vous allez devoir trouver comment faire.

Il est fort possible que les performances de l'IA soit médiocre mais je veux seulement qu'elle soit fonctionnelle.

## Configuration spécifique du serveur

Voir [1.1 - Contexte - Directives générales.md](1.1%20-%20Contexte%20-%20Directives%20g%C3%A9n%C3%A9rales.md)

## Service à installer
- Open WebUI
- Docker 

*La liste de ces services n'est pas exhaustive, il peut y avoir des dépendances.*

## Tâches après installation
- Créer un utilisateur.
- Installer le modèle Llama 3.2 1B
  - Taille : ~1.3 GB
  - RAM requise : 2-3 GB
  - Très rapide, idéal pour les tests

## Validations et tests
- Accès à Open WebUi- 
- Fonctionnalité du système
    - Tests d'envoi et réception
    - Tests des fonctionnalités d'un compte mail
    - Test d'administration

*Liste non exhaustive, d'autres tests peuvent être nécessaire*